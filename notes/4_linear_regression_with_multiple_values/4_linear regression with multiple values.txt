scalling or normalization makes gradient descent faster. because it's a better way to treating all features equally. (all mapped to a same or similar range, such as -1 to 1)

mean normalization

xi = (xi-average)/range or std.


automatic convergence test (a threshold) or plot

why plot is better? because it make sure no overshoot.(some overshoot could converge)

choose good features can lead to better regression

choose a good alpha by test and trial

polynomial regression
--------------------------
a quadratic hypothesis is a bowl shape, so be careful. (is something going up eventually going down)



normal equation (analytical method)
------------------------------------
inverse a matrix is O(n3)
disadv of normal equation is slow to deal with large n
but it doesn't need feature scalling
use it if n <10000(roughly)



if XXt is singular, then probably: 1) redundant features(linearly dependent) or m <= n (in this case use regularization)

pinv() can do pseudo inverse


the rationale behind it: partial derivative of J(theta) all = 0.
or 最小二乘法