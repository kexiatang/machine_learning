feedforward propagation is the calculation process
back propagation is the algorithm to get partial derivatives


2 ways good for debuggin:
1.plot J(theta) as function of theta to view the curve decreasing
2.gradient checking



a * delta is only the gadient for one record. for m recourd, we need to accumulate delta (to get Delta) and dived by m 

delta is actually the partial derivative of cost(i) on z where i is the record number. while z'(x) is a, so J'(theta) is a * delta in case of only one record.


epsilon 10e-4 is good, because if big, not accurate, if too small, underflow problem.

one-side difference estimate is not as good as two-sides difference


symmetry breaking



put it together
---------------
default 1 hidden layer
if n hidden layer, each layer has same numbre of unit (reasonalble default)
the more hidden unit the better (given complexity permit) usually comparable with number of features


J(theta) is NOT convex, we only can get local optimum, but it's fine here.

Notice that in ex3, oneVsAll.m we minimize J(theta) for each logistic regression seperately, but in neural networks we calculate J(theta) as a whole. if we treat multi-logistic classifier as a simple neural network of 2 layers, then if each logistic regression is optimized then the whole cost is minimized for sure. but in a general neural network all thetas are mutually dependent except the last layer, so we need to conisder overall effect as a whole.


----------------------
all-0 theta initalization will cause all the unit in the output layer to be the same therefore has the same probability for every class. even it gets a local minimum finally, the result in the output layer is still equally-likely for each class, that's why the accuracy will be low. this local minimum is undesired.


epsilon: for gradient checking, 10e-4, for random initialization, refer to the formular in ex4.

for details about gradient checking check ex4

in ex4, pay attention that visualize hidden layer is actually visualise theta, becase that's the weight for each pixel. and it has totally different meaning from the visualization of X

think about a 2 layer network this way: as the min cost is got, the result theta will try best to predict the number as it is. so for number 1, the theta that multiplicating the pixel values of 1 will be large so that z will be large for that class in order to get an output close to 1. so for those characteristic pixels of number 1, theta value will be high.

what does activate a hidden unit mean.

try to change 
displayData(Theta1(:, 2:end) );
to
displayData(Theta1(:, 2:end) .* (ones(25,1) * X(1000,:)) );

to see what happen. thus better understand why visualize theta as detectors.

also try disable random initalization, may find that all image will be the same.