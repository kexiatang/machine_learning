underfitting (high bias)
jist right
overfitting (high variance)


overfitting: fit the training set very well, but fail to generalize to new examples.

by convention we don't penalize theta0, but even it's included in the regularization terms, the solution is similar.


addressing overfitting
1. reduce number of features (model selection algorithm or manually)
2. regularization


lamda used to control the trade of between underfitting and overfitting


the new partial derivateive of theta looks like shrinking theta towards 0 each time,(trying hard to keep the absolute value of theta small) however the cost function is different, althought they minus the same term, the final optimal point is different.(they are walking along different convex )

note: a negative lambda won't necessarily provide overfitting, theta could also be decreased (look into the formula and think)