caveat:
C is not equal to 1/lambda, but when C = 1/lambda, those 2 equations give the same optimized theta.
compared to logisitc regression, 1/m is canceled, although this will not affact theta, but it no longer stand for the average error.

if C is large, we are motivated to pay more attention to optimize the first part rather than the regularization part, but still, all theta except theta0 will be regularized.


when descision boundary of SVM has a perfect margin, the total cost can be 0 (without regularization term)

because in SVM the costfunction is directly get from z, wheras in logistic regression the cost function is based on hypothesis, so in SVM hypothesis dosen't need to be sigmoid, it can be simplified just for prediction.




margin rationale:
-----------------------
in the third vedio, the math rationale is actually proving SVM will maximize margin even when all samples are satisfied by 0 cost.

in the cost function, the initial margin is determined by the corner point (1 and -1) in this case. try to draw the lines and see how the corner point related to margin. when all samples satisfiy that margin, then SVM will maximize margin because of the regularization term.

that's why when C is large, the first term tends to be 0, the decision boundary will try it's best to seperate all sample. when C is small, SVM tends to ignore outliers.(this is general case, still depend on data?)



theta'* M * theta: implementation for better computational speed for large traning set (large amount of features).
kernals can be applied for logistic regression, but the computational simplification only applies for SVM.
Therefore using kernals in logistic regression will be to slow.
there's existing software dealing with optimizing the SVM cost function.




user an SVM
---------------
library: liblinear, libsvm
choose C
choose kernal: 
-linear kernal when m small n large (preventing overfitting)
-gaussian kernal (do feature scalling before use it.)

kernal chosen must satisfy Mercer's Theorem
-polynomial kernal: (x'l+constant)^degree  (usually worse than gaussian. used when x and l are strictly non-negative) when x and l are close the inner product will be large.
- more kernals

multiclass: run 1-vs-all, and pick the biggerst theta'x.
SVM cost function is convex. no local minimum


logistic regression vs SVM
------------------------------
if n(10000) large(relative to m(10-1000)) use logistic or SVM without kernal. (prevent over fitting)
if n is small(1-1000), m is intermediate(10-10000), use SVM with gaussian kernal.
if n is small(1-1000), m is large(50000), create more features with logistic regression, or SVM without kernal.

feature scalling is necessary when different feature has significant different range to make the equally important to prevent one feature dominating the result.




review questions
-----------------
A linearly separable dataset can usually be separated by many different lines. Varying the parameter C will cause the SVM's decision boundary to vary among these possibilities. For example, for a very large value of C, it might learn larger values of ¦È in order to increase the margin on certain examples.



q: 
-------------------------
does SVM sometimes has multiple(infinity) number of optimal solution?

when there's case that some samples are really far from decision boundary, SVM is better than logistic regression because it only conisder margin, everything over the margin will have 0 cost, however logistic regression boundary will be affacted by that far sample and lead to smaller margin.(in the case without regularization)
