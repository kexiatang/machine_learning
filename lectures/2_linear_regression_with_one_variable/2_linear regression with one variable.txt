cost function, hypothesis



linear regression cost function is always a bowl shape (convex function -> only one global optimum)


gradient descent (more scallable than normal equation)

different starting point ,even very close, may lead to different local optimum value.

alpha: learning rate.(can't be too large, which could diverge.) but once is not too large, it can be constant, the actually step is gonna auto-decrease due to the slop change, and finally converge.

batch gradient descent



Q:
-----
how to determine alpha?
how to prevent local optimum


batch gradient descent use all data set for each step
another is stochastic gradient descent.