
1
00:00:00,046 --> 00:00:01,076
In previous videos, we talked

2
00:00:02,002 --> 00:00:03,064
about the gradient descent algorithm

3
00:00:04,033 --> 00:00:05,046
and talked about the linear

4
00:00:05,071 --> 00:00:08,036
regression model and the squared error cost function.

5
00:00:09,044 --> 00:00:10,059
In this video, we're going to

6
00:00:10,069 --> 00:00:12,027
put together gradient descent with

7
00:00:12,071 --> 00:00:14,052
our cost function, and that

8
00:00:14,066 --> 00:00:16,007
will give us an algorithm for

9
00:00:16,037 --> 00:00:18,050
linear regression for fitting a straight line to our data.

10
00:00:20,075 --> 00:00:22,053
So, this is

11
00:00:22,067 --> 00:00:24,041
what we worked out in the previous videos.

12
00:00:24,089 --> 00:00:26,087
That's our gradient descent algorithm, which

13
00:00:27,023 --> 00:00:28,085
should be familiar, and you

14
00:00:29,001 --> 00:00:29,094
see the linear linear regression model

15
00:00:30,071 --> 00:00:35,027
with our linear hypothesis and our squared error cost function.

16
00:00:36,057 --> 00:00:37,086
What we're going to do is apply

17
00:00:38,050 --> 00:00:40,096
gradient descent to minimize

18
00:00:44,050 --> 00:00:45,046
our squared error cost function.

19
00:00:47,072 --> 00:00:48,082
Now, in order to apply

20
00:00:49,021 --> 00:00:50,050
gradient descent, in order

21
00:00:50,068 --> 00:00:51,086
to write this piece of

22
00:00:51,096 --> 00:00:53,064
code, the key term

23
00:00:54,002 --> 00:00:56,085
we need is this derivative term over here.

24
00:00:59,052 --> 00:01:00,038
So, we need to figure out

25
00:01:00,060 --> 00:01:02,017
what is this partial derivative term,

26
00:01:02,078 --> 00:01:04,040
and plug in the

27
00:01:04,048 --> 00:01:06,003
definition of the cost

28
00:01:06,039 --> 00:01:07,093
function J, this turns

29
00:01:08,031 --> 00:01:11,054
out to be this "inaudible"

30
00:01:13,043 --> 00:01:15,048
equals 1-3 M of

31
00:01:15,068 --> 00:01:17,096
this squared error

32
00:01:20,009 --> 00:01:21,060
cost function term, and all

33
00:01:21,079 --> 00:01:22,073
I did here was I just

34
00:01:23,046 --> 00:01:25,006
you know plugged in the definition of

35
00:01:25,015 --> 00:01:27,035
the cost function there, and simplifying

36
00:01:27,095 --> 00:01:29,081
little bit more, this turns

37
00:01:30,015 --> 00:01:33,026
out to be equal to, this

38
00:01:33,082 --> 00:01:36,010
"inaudible" equals 1-3 M

39
00:01:37,015 --> 00:01:38,098
of course either one, XI

40
00:01:40,090 --> 00:01:42,071
minus YI squared.

41
00:01:43,028 --> 00:01:44,043
And all I did there was took

42
00:01:44,060 --> 00:01:46,014
the definition for my hypothesis

43
00:01:47,043 --> 00:01:48,006
and plug that in there.

44
00:01:48,090 --> 00:01:50,070
And it turns out we need

45
00:01:50,082 --> 00:01:51,064
to figure out what is

46
00:01:52,003 --> 00:01:53,012
the partial derivative of two

47
00:01:53,034 --> 00:01:54,082
cases for J equals

48
00:01:55,068 --> 00:01:57,001
0 and for J equals 1 want

49
00:01:57,025 --> 00:01:58,029
to figure out what is this

50
00:01:58,048 --> 00:02:00,053
partial derivative for both the

51
00:02:00,059 --> 00:02:03,015
theta(0) case and the theta(1) case.

52
00:02:03,093 --> 00:02:05,095
And I'm just going to write out the answers.

53
00:02:06,085 --> 00:02:10,000
It turns out this firstterm simplifies

54
00:02:10,069 --> 00:02:13,074
to 1/M, sum over

55
00:02:14,022 --> 00:02:16,009
my training set of just

56
00:02:16,080 --> 00:02:20,003
that, X(i)-  Y(i).

57
00:02:21,024 --> 00:02:22,052
And for this term, partial derivative

58
00:02:23,062 --> 00:02:25,065
with respect to theta(1), it turns

59
00:02:26,012 --> 00:02:30,034
out I get this term: -Y(i)*X(i).

60
00:02:32,027 --> 00:02:32,027
Okay.

61
00:02:36,008 --> 00:02:38,030
Andcomputing these partial

62
00:02:38,075 --> 00:02:40,030
derivatives, so going from

63
00:02:40,099 --> 00:02:44,012
this equation to either

64
00:02:44,040 --> 00:02:45,093
of these equations down there, computing

65
00:02:46,052 --> 00:02:49,069
those partial derivative terms requires some multivariate calculus.

66
00:02:51,012 --> 00:02:52,053
If you know calculus, feel free

67
00:02:52,091 --> 00:02:54,027
to work through the derivations yourself

68
00:02:54,071 --> 00:02:55,077
and check take the derivatives

69
00:02:56,074 --> 00:02:58,069
you actually get the answers that I got.

70
00:02:59,043 --> 00:03:00,044
But if you are less

71
00:03:00,062 --> 00:03:02,025
familiar with calculus you don't

72
00:03:02,050 --> 00:03:04,009
worry about it, and it

73
00:03:04,018 --> 00:03:05,056
is fine to take these equations

74
00:03:06,012 --> 00:03:07,069
worked out, and you

75
00:03:07,081 --> 00:03:09,015
won't need to know calculus or

76
00:03:09,022 --> 00:03:10,046
anything like that in order to

77
00:03:10,056 --> 00:03:13,000
do the homework, so to implement gradient descent you'd have to work.

78
00:03:14,061 --> 00:03:16,003
But so, after these definitions,

79
00:03:16,044 --> 00:03:17,069
or after what we've worked

80
00:03:18,000 --> 00:03:19,062
out to be the derivatives, which

81
00:03:19,075 --> 00:03:21,003
is really just the slope of

82
00:03:21,013 --> 00:03:23,033
the cos function j.  We

83
00:03:23,049 --> 00:03:24,053
can now plug them back into

84
00:03:25,003 --> 00:03:26,006
our gradient descent algorithm.

85
00:03:26,087 --> 00:03:28,049
So here's gradient descent, or

86
00:03:28,068 --> 00:03:29,084
the regression, which is going

87
00:03:29,097 --> 00:03:32,099
to repeat until convergence, theta 0

88
00:03:33,016 --> 00:03:35,008
and theta one get updated as,

89
00:03:35,037 --> 00:03:36,053
you know, the same minus alpha

90
00:03:37,018 --> 00:03:38,012
times the derivative term.

91
00:03:39,037 --> 00:03:40,063
So, this term here.

92
00:03:43,002 --> 00:03:45,059
So, here's our linear regression algorithm.

93
00:03:47,002 --> 00:03:52,055
This first term here that

94
00:03:52,081 --> 00:03:54,003
term is, of course, just

95
00:03:54,050 --> 00:03:55,091
a posh derivative of respective

96
00:03:56,006 --> 00:03:58,046
theta zero, that we worked on in the previous slide.

97
00:03:59,081 --> 00:04:01,031
And this second term here,

98
00:04:01,090 --> 00:04:04,008
that term is just

99
00:04:04,040 --> 00:04:06,000
a partial derivative with respect to

100
00:04:06,012 --> 00:04:10,043
theta one that we worked out on the previous line.

101
00:04:11,037 --> 00:04:12,011
And just as a quick reminder,

102
00:04:13,030 --> 00:04:14,084
you must, when implementing gradient descent,

103
00:04:15,031 --> 00:04:16,066
there's actually there's detail that, you

104
00:04:16,075 --> 00:04:18,013
know, you should be implementing

105
00:04:18,068 --> 00:04:21,030
it so the update theta zero and theta one simultaneously.

106
00:04:24,023 --> 00:04:26,086
So, let's see how gradient descent works.

107
00:04:28,000 --> 00:04:29,001
One of the issues we solved

108
00:04:29,036 --> 00:04:31,081
gradient descent is that it can be susceptible to local optima.

109
00:04:32,076 --> 00:04:33,081
So, when I first explained gradient

110
00:04:34,008 --> 00:04:35,000
descent, I showed you this picture

111
00:04:35,092 --> 00:04:36,089
of it, you know, going downhill

112
00:04:37,043 --> 00:04:38,062
on the surface and we

113
00:04:38,077 --> 00:04:39,087
saw how, depending on where

114
00:04:40,006 --> 00:04:41,097
you're initializing, you can end up with different local optima.

115
00:04:43,052 --> 00:04:44,024
You know, you can end up here or here.

116
00:04:45,042 --> 00:04:46,083
But, it turns out that

117
00:04:47,018 --> 00:04:48,094
the cost function for gradient

118
00:04:49,043 --> 00:04:50,062
of cost function for linear regression

119
00:04:51,072 --> 00:04:52,079
is always going to be

120
00:04:53,000 --> 00:04:54,060
a bow-shaped function like this.

121
00:04:55,097 --> 00:04:57,008
The technical term for this

122
00:04:57,041 --> 00:04:59,079
is that this is called a convex function.

123
00:05:03,023 --> 00:05:04,093
And I'm not going

124
00:05:05,000 --> 00:05:06,029
to give the formal definition for what

125
00:05:06,054 --> 00:05:09,042
is a convex function, c-o-n-v-e-x, but

126
00:05:09,060 --> 00:05:10,087
informally a convex function

127
00:05:11,037 --> 00:05:14,045
means a bow-shaped function, you know, kind of like a bow shaped.

128
00:05:15,074 --> 00:05:17,062
And so, this function doesn't

129
00:05:17,095 --> 00:05:19,050
have any local optima, except

130
00:05:20,025 --> 00:05:21,018
for the one global optimum.

131
00:05:22,033 --> 00:05:23,062
And does gradient descent on

132
00:05:24,023 --> 00:05:25,050
this type of cost function which

133
00:05:25,068 --> 00:05:27,024
you get whenever you're using linear

134
00:05:27,045 --> 00:05:28,067
regression, it will always convert

135
00:05:29,027 --> 00:05:31,051
to the global optimum, because there are no other local optima other than global optimum.

136
00:05:34,027 --> 00:05:36,022
So now, let's see this algorithm in action.

137
00:05:38,020 --> 00:05:39,074
As usual, zero plus of

138
00:05:40,005 --> 00:05:42,017
the hypothesis function and of

139
00:05:42,044 --> 00:05:44,043
my cost function J.

140
00:05:45,087 --> 00:05:46,082
And so, let's see how

141
00:05:47,006 --> 00:05:48,092
to initialize my parameters at this value.

142
00:05:49,087 --> 00:05:51,068
You know, let's say, usually you

143
00:05:51,088 --> 00:05:53,031
initialize your parameters at zero

144
00:05:53,057 --> 00:05:55,042
for zero, theta zero and zero.

145
00:05:56,018 --> 00:05:58,032
For illustration in this

146
00:05:58,051 --> 00:06:00,099
specific presentation, I have

147
00:06:01,030 --> 00:06:02,074
initialised theta zero at

148
00:06:03,000 --> 00:06:05,056
about 900, and theta one at about minus 0.1, okay?

149
00:06:06,072 --> 00:06:09,024
And so, this corresponds to H

150
00:06:09,055 --> 00:06:11,055
over X, equals, you know,

151
00:06:11,085 --> 00:06:13,029
minus 900 minus 0.1 x

152
00:06:13,037 --> 00:06:17,098
is this line, so out here on the cost function.

153
00:06:19,027 --> 00:06:20,019
Now if we take one

154
00:06:20,043 --> 00:06:22,006
step of gradient descent, we end

155
00:06:22,019 --> 00:06:23,087
up going from this point

156
00:06:24,016 --> 00:06:26,031
out here, a little

157
00:06:27,012 --> 00:06:28,055
bit to the down left

158
00:06:29,031 --> 00:06:30,047
to that second point over there.

159
00:06:31,036 --> 00:06:34,069
And, you notice that my line changed a little bit.

160
00:06:35,012 --> 00:06:36,016
And, as I take another step

161
00:06:36,036 --> 00:06:39,019
at gradient descent, my line on the left will change.

162
00:06:41,016 --> 00:06:41,016
Right.

163
00:06:41,045 --> 00:06:42,099
And I have also

164
00:06:43,056 --> 00:06:45,089
moved to a new point on my cost function.

165
00:06:47,060 --> 00:06:48,058
And as I think further step

166
00:06:48,093 --> 00:06:50,064
is gradient descent, I'm going

167
00:06:50,093 --> 00:06:52,072
down in cost, right, so

168
00:06:52,098 --> 00:06:54,067
my parameter is following

169
00:06:55,008 --> 00:06:57,086
this trajectory, and if

170
00:06:57,095 --> 00:06:59,075
you look on the left, this corresponds

171
00:07:00,052 --> 00:07:02,098
to hypotheses that seem

172
00:07:03,073 --> 00:07:04,069
to be getting to be

173
00:07:04,087 --> 00:07:06,000
better and better fits for the

174
00:07:06,006 --> 00:07:08,047
data until eventually,

175
00:07:10,000 --> 00:07:11,088
I have now wound up at the global minimum.

176
00:07:13,000 --> 00:07:15,087
And this global minimum corresponds to

177
00:07:16,012 --> 00:07:19,073
this hypothesis, which gives me a good fit to the data.

178
00:07:21,037 --> 00:07:23,006
And so that's gradient

179
00:07:23,066 --> 00:07:24,076
descent, and we've just run

180
00:07:24,093 --> 00:07:26,066
it and gotten a good

181
00:07:26,081 --> 00:07:29,099
fit to my data set of housing prices.

182
00:07:31,006 --> 00:07:33,000
And you can now use it to predict.

183
00:07:34,005 --> 00:07:35,013
You know, if your friend has a

184
00:07:35,017 --> 00:07:37,036
house with a

185
00:07:37,058 --> 00:07:38,077
size 1250 square feet, you

186
00:07:38,099 --> 00:07:39,093
can now read off the value

187
00:07:40,041 --> 00:07:41,080
and tell them that, I don't

188
00:07:41,098 --> 00:07:42,082
know, maybe they can get

189
00:07:43,043 --> 00:07:45,048
$350,000 for their house.

190
00:07:48,067 --> 00:07:49,068
Finally, just to give

191
00:07:49,079 --> 00:07:51,017
this another name, it turns out

192
00:07:51,043 --> 00:07:52,080
that the algorithm that we

193
00:07:52,088 --> 00:07:54,025
just went over is sometimes

194
00:07:55,008 --> 00:07:56,033
called batch gradient descent.

195
00:07:57,050 --> 00:07:58,047
And it turns out in machine

196
00:07:58,074 --> 00:08:00,008
learning, I feel like us machine

197
00:08:00,037 --> 00:08:01,041
learning people, we're not always

198
00:08:01,095 --> 00:08:03,017
created has given me some algorithms.

199
00:08:04,025 --> 00:08:05,083
But the term batch gradient descent

200
00:08:06,061 --> 00:08:07,089
means that refers to the

201
00:08:07,093 --> 00:08:09,023
fact that, in every step

202
00:08:09,044 --> 00:08:11,006
of gradient descent we're looking

203
00:08:11,044 --> 00:08:12,066
at all of the training examples.

204
00:08:13,079 --> 00:08:15,056
So, in gradient descent, you

205
00:08:15,064 --> 00:08:17,083
know, when computing derivatives, we're computing

206
00:08:18,043 --> 00:08:19,039
these sums, this sum of.

207
00:08:19,093 --> 00:08:21,083
So, in every separate

208
00:08:22,011 --> 00:08:23,012
gradient descent, we end up

209
00:08:23,023 --> 00:08:24,087
computing something like this, that

210
00:08:25,014 --> 00:08:27,031
sums over our M training examples.

211
00:08:28,042 --> 00:08:29,050
And so the term batch gradient

212
00:08:29,087 --> 00:08:31,007
descent refers to the fact

213
00:08:31,016 --> 00:08:32,070
when looking at the entire batch

214
00:08:33,001 --> 00:08:34,028
of training examples, and again,

215
00:08:34,072 --> 00:08:35,058
this is really, really not

216
00:08:35,065 --> 00:08:36,062
a great name, but this is

217
00:08:36,075 --> 00:08:38,055
what Mission Learning people call it.

218
00:08:39,057 --> 00:08:41,001
And it turns out there are

219
00:08:41,029 --> 00:08:42,072
sometimes other versions of

220
00:08:42,079 --> 00:08:43,086
gradient descent that are not

221
00:08:44,008 --> 00:08:46,021
back versions but instead do

222
00:08:46,033 --> 00:08:47,025
not look at the entire trading

223
00:08:48,022 --> 00:08:49,052
but look at small subsets

224
00:08:50,005 --> 00:08:51,007
of the training sets at the time,

225
00:08:51,074 --> 00:08:54,036
and we'll talk about those versions later in this course as well.

226
00:08:55,014 --> 00:08:56,005
But for now, using the algorithm

227
00:08:56,011 --> 00:08:57,009
you just learned, now we're

228
00:08:57,025 --> 00:08:59,009
using batch gradient descent, you

229
00:08:59,025 --> 00:09:00,066
now know how to implement

230
00:09:01,030 --> 00:09:02,098
gradient descent, or linear regression.

231
00:09:05,099 --> 00:09:08,026
So that's linear regression with gradient descent.

232
00:09:09,046 --> 00:09:11,022
If you've seen advanced linear algebra

233
00:09:11,075 --> 00:09:12,058
before so some you may

234
00:09:12,069 --> 00:09:13,079
have taken a class with advanced

235
00:09:14,013 --> 00:09:15,076
linear algebra, you might

236
00:09:16,003 --> 00:09:17,001
know that there exists a solution

237
00:09:18,012 --> 00:09:19,076
for numerically solving for the

238
00:09:19,087 --> 00:09:20,077
minimum of the cost function

239
00:09:21,021 --> 00:09:22,058
J, without needing to

240
00:09:22,067 --> 00:09:24,097
use and iterative algorithm like gradient descent.

241
00:09:25,080 --> 00:09:26,079
Later in this course we will

242
00:09:27,011 --> 00:09:28,007
talk about that method as

243
00:09:28,021 --> 00:09:29,066
well that just solves for the

244
00:09:29,075 --> 00:09:31,013
minimum cost function J without

245
00:09:31,040 --> 00:09:33,058
needing this multiple steps of gradient descent.

246
00:09:34,046 --> 00:09:36,087
That other method is called normal equations methods.

247
00:09:37,069 --> 00:09:38,087
And, but in case you

248
00:09:38,096 --> 00:09:39,091
have heard of that method, it turns

249
00:09:40,022 --> 00:09:41,070
out gradient descent will

250
00:09:41,089 --> 00:09:43,046
scale better to larger data

251
00:09:43,070 --> 00:09:44,099
sets than that normal equals

252
00:09:45,033 --> 00:09:47,019
method and, now that

253
00:09:47,030 --> 00:09:48,066
we know about gradient descent, we'll

254
00:09:48,087 --> 00:09:49,072
be able to use it in

255
00:09:50,002 --> 00:09:51,037
lots of different contexts, and we'll

256
00:09:51,046 --> 00:09:53,062
use it in lots of different Mission Learning problems as well.

257
00:09:55,024 --> 00:09:57,008
So, congrats on learning

258
00:09:57,049 --> 00:09:59,009
about your first Mission Learning algorithm.

259
00:10:00,012 --> 00:10:02,024
We'll later have exercises in

260
00:10:02,038 --> 00:10:03,028
which we'll ask you to

261
00:10:03,048 --> 00:10:04,087
implement gradient descent and

262
00:10:05,002 --> 00:10:06,040
hopefully see these algorithms work for yourselves.

263
00:10:07,041 --> 00:10:08,055
But before that I first

264
00:10:08,092 --> 00:10:10,022
want to tell you in

265
00:10:10,030 --> 00:10:11,038
the next set of videos, the

266
00:10:11,050 --> 00:10:12,088
first want to tell you about

267
00:10:13,021 --> 00:10:14,044
a generalization of the gradient descent

268
00:10:15,017 --> 00:10:16,032
algorithm that will make

269
00:10:16,045 --> 00:10:18,007
it much more powerful and I

270
00:10:18,013 --> 00:10:20,020
guess I will tell you about that in the next video.
